{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Practical Guide to Running Object Detection Models: YOLOX Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a practical guide for running object detection models on Tenstorrent hardware devices using the TT-BUDA compiler stack. *For detailed information on model compatibility, please refer to the [models support table](#) to check which model works with which Tenstorrent device(s).*\n",
    "\n",
    "In this example, we demonstrate how to use the [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) model on Tenstorrent AI accelerator hardware to label objects in a video file. The input file `video.mp4` is processed, and the output with labeled objects is saved as `output_video.mp4`.\n",
    "\n",
    "**Note on terminology:**\n",
    "\n",
    "While TT-BUDA is the official Tenstorrent AI/ML compiler stack, PyBUDA is the Python interface for TT-BUDA. TT-BUDA is the core technology; however, PyBUDA allows users to access and utilize TT-BUDA's features directly from Python. This includes directly importing model architectures and weights from PyTorch, TensorFlow, ONNX, and TFLite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide Overview\n",
    "\n",
    "In this guide, we cover the steps for running the **YOLOX** model to detect objects in a video. The results are saved in a new video file with bounding boxes and labels overlaid on detected objects.\n",
    "\n",
    "You will learn how to:\n",
    "- Set up the appropriate libraries and environment\n",
    "- Run YOLOX for video labeling\n",
    "- Save the output with detected labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries\n",
    "\n",
    "Make sure that you have an activate Python environment with the latest version of PyBUDA installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pybuda\n",
    "import requests\n",
    "import torch\n",
    "import logging\n",
    "from queue import Queue\n",
    "from pybuda._C.backend_api import BackendDevice\n",
    "from yolox.data.data_augment import preproc as preprocess\n",
    "from yolox.data.datasets import COCO_CLASSES\n",
    "from yolox.exp import get_exp\n",
    "from yolox.utils import demo_postprocess, multiclass_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure PyBUDA Parameters and Device-Specific Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are optional configurations that can be adjusted before compiling and running a model on Tenstorrent hardware. Sometimes, the configurations are necessary to compile the model and other times they are tuneable parameters that can be adjusted for performance enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolox_video(variant, video_path, output_path, batch_size=1):\n",
    "    \"\"\"\n",
    "    Process all frames of a video file using YOLOX and generate a new video file containing object detection results.\n",
    "    param variant: YOLOX model variant (e.g., yolox_nano, yolox_tiny, yolox_s, yolox_m, yolox_l, yolox_x)\n",
    "    param video_path: input video file path\n",
    "    param output_path: output video file path\n",
    "\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s | %(levelname)s | %(message)s')\n",
    "    logging.info(\"Starting YOLOX video processing.\")\n",
    "\n",
    "     # Set PyBuda configuration parameters\n",
    "    compiler_cfg = pybuda.config._get_global_compiler_config()\n",
    "    compiler_cfg.balancer_policy = \"Ribbon\"\n",
    "    compiler_cfg.default_df_override = pybuda.DataFormat.Float16_b\n",
    "    os.environ[\"PYBUDA_DECOMPOSE_SIGMOID\"] = \"1\"\n",
    "    logging.info(\"PyBuda configuration set.\")\n",
    "\n",
    "    # Device specific configurations\n",
    "    available_devices = pybuda.detect_available_devices()\n",
    "    logging.info(f\"Available devices: {available_devices}\")\n",
    "    if available_devices:\n",
    "        if available_devices[0] == BackendDevice.Wormhole_B0:\n",
    "            if variant not in [\"yolox_nano\", \"yolox_s\"]:\n",
    "                os.environ[\"PYBUDA_FORK_JOIN_BUF_QUEUES\"] = \"1\"\n",
    "                os.environ[\"PYBUDA_FORK_JOIN_EXPAND_OUTPUT_BUFFERS\"] = \"1\"\n",
    "                os.environ[\"PYBUDA_FORK_JOIN_SKIP_EXPANDING_BUFFERS\"] = \"1\"\n",
    "                os.environ[\"TT_BACKEND_TIMEOUT\"] = \"7200\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Model Weights and Prepare the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loads the YOLOX model weights if not already available, loads the weights into the model, and compiles the model with PyBUDA for the specified variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_name = f\"weight/{variant}.pth\"\n",
    "model_name = \"yolov3\" if variant == \"yolox_darknet\" else variant.replace(\"_\", \"-\")\n",
    "exp = get_exp(exp_name=model_name)\n",
    "model = exp.get_model()\n",
    "ckpt = torch.load(weight_name, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "tt_model = pybuda.PyTorchModule(f\"pt_{variant}\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Dummy Input and Compile Model\n",
    "\n",
    "A dummy input is created to initialize and test the model on the hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (416, 416) if variant in [\"yolox_nano\", \"yolox_tiny\"] else (640, 640)\n",
    "dummy_input = torch.randn(1, 3, input_shape[0], input_shape[1])\n",
    "try:\n",
    "    dummy_output_queue = pybuda.run_inference(tt_model, inputs=[(dummy_input, )], input_count=batch_size)\n",
    "    dummy_output = dummy_output_queue.get()\n",
    "    logging.info(\"Model compiled successfully with dummy inference.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Model compilation failed: {e}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Open Video and Initialize Video Writer\n",
    "\n",
    "The video is opened for reading, and an output video writer is initialized to save the processed frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    logging.error(f\"Failed to open video file {video_path}\")\n",
    "    return\n",
    "width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(output_path, fourcc, original_fps, (width, height))\n",
    "logging.info(f\"Output video writer initialized: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Process Video Frames\n",
    "\n",
    "Each frame is read, preprocessed, and passed to the model for inference. Detected objects are drawn on each frame, and the frame is saved to the output video.\n",
    "\n",
    " ### **Run inference on the targeted device**\n",
    "\n",
    "\n",
    "Running a model on a Tenstorrent device invovles two parts: compilation and runtime.\n",
    "\n",
    "Compilation -- TT-BUDA is a compiler. Meaning that it will take a model architecture graph and compile it for the target hardware. Compilation can take anywhere from a few seconds to a few minutes, depending on the model. This only needs to happen once. When you execute the following block of code the compilation logs will be displayed.\n",
    "\n",
    "Runtime -- once the model has been compiled and loaded onto the device, the user can push new inputs which will execute immediately.\n",
    "\n",
    "The `run_inference` API can achieve both steps in a single call. If it's the first call, the model will compile. Any subsequent calls will execute runtime only.\n",
    "\n",
    "Please refer to the documentation for alternative APIs such as `initialize_pipeline` and `run_forward`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            logging.info(\"End of video file reached.\")\n",
    "            break\n",
    "        frame_count += 1\n",
    "        logging.info(f\"Processing frame {frame_count}\")\n",
    "\n",
    "        # Image preprocessing\n",
    "        img, ratio = preprocess(frame, input_shape)\n",
    "        img_tensor = torch.from_numpy(img).unsqueeze(0).float()  # (1, 3, H, W)\n",
    "        batch_input = torch.cat([img_tensor] * batch_size, dim=0)  # (batch_size, 3, H, W)\n",
    "\n",
    "        try:\n",
    "            \n",
    "             # Run inference on Tenstorrent device\n",
    "            logging.debug(f\"Before run_inference call for frame {frame_count}\")\n",
    "            output_queue = pybuda.run_inference(None, inputs=[(batch_input, )], input_count=batch_size)\n",
    "            output = output_queue.get()\n",
    "            logging.debug(f\"After run_inference call for frame {frame_count}\")\n",
    "            logging.debug(f\"Inference completed for frame {frame_count}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Inference failed for frame {frame_count}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Output processing\n",
    "            if isinstance(output, list):\n",
    "                output = output[0]\n",
    "            if isinstance(output, pybuda.Tensor):\n",
    "                output_array = output.value().detach().float().numpy()\n",
    "            elif isinstance(output, torch.Tensor):\n",
    "                output_array = output.detach().float().numpy()\n",
    "            else:\n",
    "                logging.error(f\"Unexpected output type: {type(output)}\")\n",
    "                continue\n",
    "\n",
    "            # Check output array shape\n",
    "            logging.debug(f\"Output array shape: {output_array.shape}\")\n",
    "            logging.debug(f\"Output array dtype: {output_array.dtype}\")\n",
    "            logging.debug(f\"Output array stats: min={np.min(output_array)}, max={np.max(output_array)}, mean={np.mean(output_array)}\")\n",
    "\n",
    "            # If the output array is 1-dimensional, reshape it to 2-dimensional\n",
    "            if output_array.ndim == 1:\n",
    "                logging.warning(f\"Output array is 1-dimensional for frame {frame_count}. Reshaping to 2D.\")\n",
    "                output_array = output_array.reshape(1, -1)\n",
    "                logging.debug(f\"Reshaped output array shape: {output_array.shape}\")\n",
    "\n",
    "            # Check for NaN or Inf values in the output array\n",
    "            if np.isnan(output_array).any() or np.isinf(output_array).any():\n",
    "                logging.error(f\"Invalid values (NaN or Inf) in output_array for frame {frame_count}\")\n",
    "                continue\n",
    "\n",
    "            # Post-processing \n",
    "            predictions = demo_postprocess(output_array, input_shape)[0]\n",
    "            if predictions is None or predictions.shape[0] == 0:\n",
    "                logging.info(f\"No objects detected in frame {frame_count}.\")\n",
    "                out.write(frame)  \n",
    "                continue\n",
    "\n",
    "            boxes = predictions[:, :4]\n",
    "            scores = predictions[:, 4:5] * predictions[:, 5:]\n",
    "            boxes_xyxy = np.ones_like(boxes)\n",
    "            boxes_xyxy[:, 0] = boxes[:, 0] - boxes[:, 2] / 2.0\n",
    "            boxes_xyxy[:, 1] = boxes[:, 1] - boxes[:, 3] / 2.0\n",
    "            boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2] / 2.0\n",
    "            boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3] / 2.0\n",
    "            boxes_xyxy /= ratio\n",
    "\n",
    "            dets = multiclass_nms(\n",
    "                boxes_xyxy, scores, nms_thr=0.45, score_thr=0.1\n",
    "            )\n",
    "            logging.debug(f\"Post-processing completed for frame {frame_count}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Post-processing failed for frame {frame_count}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        #score_threshold = 0.30\n",
    "\n",
    "        if dets is not None:\n",
    "            final_boxes, final_scores, final_cls_inds = dets[:, :4], dets[:, 4], dets[:, 5]\n",
    "            for box, score, cls_ind in zip(final_boxes, final_scores, final_cls_inds):\n",
    "\n",
    "                #if score < score_threshold:\n",
    "                #    continue\n",
    "\n",
    "                class_name = COCO_CLASSES[int(cls_ind)]\n",
    "                x_min, y_min, x_max, y_max = map(int, box)\n",
    "               \n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (x_min, y_min),\n",
    "                    (x_max, y_max),\n",
    "                    (255, 0, 255),\n",
    "                    1,\n",
    "                )\n",
    "                \n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"{class_name} {score:.2f}\",\n",
    "                    (x_min, y_min - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (255, 0, 255),\n",
    "                    2,\n",
    "                )\n",
    "                \n",
    "\n",
    "        \n",
    "        out.write(frame)\n",
    "        logging.debug(f\"Frame {frame_count} written to output video.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Release Resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "logging.info(\"Video processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Result**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/eSlimKorea/Model-TEST-TT-BUDA/blob/main/Model%20Test/CV_Models/YOLOX/output_1.jpg?raw=true\" width=\"600\" style=\"margin: 10px;\">\n",
    "    <img src=\"https://github.com/eSlimKorea/Model-TEST-TT-BUDA/blob/main/Model%20Test/CV_Models/YOLOX/output_2.jpg?raw=true\" width=\"600\" style=\"margin: 10px;\">\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/eSlimKorea/Model-TEST-TT-BUDA/blob/main/Model%20Test/CV_Models/YOLOX/output_3.jpg?raw=true\" width=\"600\" style=\"margin: 10px;\">\n",
    "    <img src=\"https://github.com/eSlimKorea/Model-TEST-TT-BUDA/blob/main/Model%20Test/CV_Models/YOLOX/output_4.jpg?raw=true\" width=\"600\" style=\"margin: 10px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
