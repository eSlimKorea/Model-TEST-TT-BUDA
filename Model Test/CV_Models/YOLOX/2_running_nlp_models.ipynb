{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Practical Guide to Running Object Detection Models: YOLOX Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a practical guide for running object detection models on Tenstorrent hardware devices using the TT-BUDA compiler stack. *For detailed information on model compatibility, please refer to the [models support table](#) to check which model works with which Tenstorrent device(s).*\n",
    "\n",
    "In this example, we demonstrate how to use the [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) model on Tenstorrent AI accelerator hardware to label objects in a video file. The input file `video.mp4` is processed, and the output with labeled objects is saved as `output_video.mp4`.\n",
    "\n",
    "**Note on terminology:**\n",
    "\n",
    "While TT-BUDA is the official Tenstorrent AI/ML compiler stack, PyBUDA is the Python interface for TT-BUDA. TT-BUDA is the core technology; however, PyBUDA allows users to access and utilize TT-BUDA's features directly from Python. This includes directly importing model architectures and weights from PyTorch, TensorFlow, ONNX, and TFLite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide Overview\n",
    "\n",
    "In this guide, we cover the steps for running the **YOLOX** model to detect objects in a video. The results are saved in a new video file with bounding boxes and labels overlaid on detected objects.\n",
    "\n",
    "You will learn how to:\n",
    "- Set up the appropriate libraries and environment\n",
    "- Run YOLOX for video labeling\n",
    "- Save the output with detected labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries\n",
    "\n",
    "Make sure that you have an activate Python environment with the latest version of PyBUDA installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.8' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/a/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pybuda\n",
    "import requests\n",
    "import torch\n",
    "import logging\n",
    "from queue import Queue\n",
    "from pybuda._C.backend_api import BackendDevice\n",
    "from yolox.data.data_augment import preproc as preprocess\n",
    "from yolox.data.datasets import COCO_CLASSES\n",
    "from yolox.exp import get_exp\n",
    "from yolox.utils import demo_postprocess, multiclass_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set PyBUDA Configuration and Device-Specific Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolox_video(variant, video_path, output_path, batch_size=1):\n",
    "    \"\"\"\n",
    "    Process all frames of a video file using YOLOX and generate a new video file containing object detection results.\n",
    "    param variant: YOLOX model variant (e.g., yolox_nano, yolox_tiny, yolox_s, yolox_m, yolox_l, yolox_x)\n",
    "    param video_path: input video file path\n",
    "    param output_path: output video file path\n",
    "\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s | %(levelname)s | %(message)s')\n",
    "    logging.info(\"Starting YOLOX video processing.\")\n",
    "\n",
    "     # Set PyBuda configuration parameters\n",
    "    compiler_cfg = pybuda.config._get_global_compiler_config()\n",
    "    compiler_cfg.balancer_policy = \"Ribbon\"\n",
    "    compiler_cfg.default_df_override = pybuda.DataFormat.Float16_b\n",
    "    os.environ[\"PYBUDA_DECOMPOSE_SIGMOID\"] = \"1\"\n",
    "    logging.info(\"PyBuda configuration set.\")\n",
    "\n",
    "    # Device specific configurations\n",
    "    available_devices = pybuda.detect_available_devices()\n",
    "    logging.info(f\"Available devices: {available_devices}\")\n",
    "    if available_devices:\n",
    "        if available_devices[0] == BackendDevice.Wormhole_B0:\n",
    "            if variant not in [\"yolox_nano\", \"yolox_s\"]:\n",
    "                os.environ[\"PYBUDA_FORK_JOIN_BUF_QUEUES\"] = \"1\"\n",
    "                os.environ[\"PYBUDA_FORK_JOIN_EXPAND_OUTPUT_BUFFERS\"] = \"1\"\n",
    "                os.environ[\"PYBUDA_FORK_JOIN_SKIP_EXPANDING_BUFFERS\"] = \"1\"\n",
    "                os.environ[\"TT_BACKEND_TIMEOUT\"] = \"7200\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Model Weights and Prepare the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loads the YOLOX model weights if not already available, loads the weights into the model, and compiles the model with PyBUDA for the specified variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_name = f\"weight/{variant}.pth\"\n",
    "model_name = \"yolov3\" if variant == \"yolox_darknet\" else variant.replace(\"_\", \"-\")\n",
    "exp = get_exp(exp_name=model_name)\n",
    "model = exp.get_model()\n",
    "ckpt = torch.load(weight_name, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "tt_model = pybuda.PyTorchModule(f\"pt_{variant}\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Dummy Input and Compile Model\n",
    "\n",
    "A dummy input is created to initialize and test the model on the hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (416, 416) if variant in [\"yolox_nano\", \"yolox_tiny\"] else (640, 640)\n",
    "dummy_input = torch.randn(1, 3, input_shape[0], input_shape[1])\n",
    "try:\n",
    "    dummy_output_queue = pybuda.run_inference(tt_model, inputs=[(dummy_input, )], input_count=batch_size)\n",
    "    dummy_output = dummy_output_queue.get()\n",
    "    logging.info(\"Model compiled successfully with dummy inference.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Model compilation failed: {e}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure PyBUDA Parameters\n",
    "\n",
    "There are optional configurations that can be adjusted before compiling and running a model on Tenstorrent hardware. Sometimes, the configurations are necessary to compile the model and other times they are tuneable parameters that can be adjusted for performance enhancement.\n",
    "\n",
    "For the BERT model, two key parameters are required for compilation:\n",
    "\n",
    "* `default_df_override`\n",
    "* `default_dram_parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PyBuda configurations\n",
    "compiler_cfg = pybuda.config._get_global_compiler_config()\n",
    "compiler_cfg.default_df_override = pybuda._C.DataFormat.Float16_b\n",
    "compiler_cfg.default_dram_parameters = False\n",
    "compiler_cfg.balancer_policy = \"Ribbon\"\n",
    "os.environ[\"PYBUDA_RIBBON2\"] = \"1\"\n",
    "os.environ[\"TT_BACKEND_OVERLAY_MAX_EXTRA_BLOB_SIZE\"] = f\"{81*1024}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Instantiate Tenstorrent device\n",
    "\n",
    "The first time we use PyBUDA, we must initialize a `TTDevice` object which serves as the abstraction over the target hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt0 = pybuda.TTDevice(\n",
    "    name=\"tt_device_0\",  # here we can give our device any name we wish, for tracking purposes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `tt0` object is created without specifying the device architecture. Pybuda will automatically detectt and define the architecture based on which Tenstorrent device its run on.\n",
    "\n",
    "### Specifying the Architecture\n",
    "If you want to specify the target device architecture, you can pass the `arch` parameter. Hereâ€™s how it can be done:\n",
    "\n",
    "```python\n",
    "# Create a TTDevice instance with a specified architecture\n",
    "tt0 = pybuda.TTDevice(\n",
    "    name=\"tt_device_0\",  # You can give your device any name for tracking purposes\n",
    "    arch=pybuda.BackendDevice.Grayskull  # Optionally set the target device architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create a PyBUDA module from PyTorch model\n",
    "\n",
    "Next, we must abstract the PyTorch model loaded from HuggingFace into a `pybuda.PyTorchModule` object. This will let the BUDA compiler know which model architecture and AI framework it has to compile.\n",
    "\n",
    "We then \"place\" this module onto the previously initialized `TTDevice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create module\n",
    "pybuda_module = pybuda.PyTorchModule(\n",
    "    name = \"pt_bert_question_answering\",  # give the module a name, this will be used for tracking purposes\n",
    "    module=model  # specify the model that is being targeted for compilation\n",
    ")\n",
    "\n",
    "# Place module on device\n",
    "tt0.place_module(module=pybuda_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Push the (tokenized) inputs into the model input queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push inputs\n",
    "tt0.push_to_inputs(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run inference on the targeted device\n",
    "\n",
    "Running a model on a Tenstorrent device invovles two parts: compilation and runtime.\n",
    "\n",
    "Compilation -- TT-BUDA is a compiler. Meaning that it will take a model architecture graph and compile it for the target hardware. Compilation can take anywhere from a few seconds to a few minutes, depending on the model. This only needs to happen once. When you execute the following block of code the compilation logs will be displayed.\n",
    "\n",
    "Runtime -- once the model has been compiled and loaded onto the device, the user can push new inputs which will execute immediately.\n",
    "\n",
    "The `run_inference` API can achieve both steps in a single call. If it's the first call, the model will compile. Any subsequent calls will execute runtime only.\n",
    "\n",
    "Please refer to the documentation for alternative APIs such as `initialize_pipeline` and `run_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on Tenstorrent device\n",
    "output_q = pybuda.run_inference()  # executes compilation (if first time) + runtime\n",
    "output = output_q.get()  # get last value from output queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Data Postprocessing\n",
    "\n",
    "Data postprocessing is done to convert the model outputs into a readable / useful format. For NLP models, this usually means receiving the logit outputs from the model, extracting the top matching tokens, and then decoding the tokens into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data postprocessing\n",
    "answer_start = output[0].value().argmax().item()\n",
    "answer_end = output[1].value().argmax().item()\n",
    "answer = tokenizer.decode(input_tokens[\"input_ids\"][0, answer_start : answer_end + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Print and evaluate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print outputs\n",
    "print(f\"Input context:\\n{context}\")\n",
    "print(f\"\\nInput question:\\n{question}\")\n",
    "print(f\"\\nOutput from model running on TTDevice:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Shutdown PyBuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pybuda.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
