# A Practical Guide to Running Object Detection Models: YOLOX Use Case

This notebook provides a practical guide for running object detection models on Tenstorrent hardware devices using the TT-BUDA compiler stack. *For detailed information on model compatibility, please refer to the [models support table](#) to check which model works with which Tenstorrent device(s).*

In this example, we demonstrate how to use the [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) model on Tenstorrent AI accelerator hardware to label objects in a video file. The input file `video.mp4` is processed, and the output with labeled objects is saved as `output_video.mp4`.



**Note on Terminology** :

While TT-BUDA is the official Tenstorrent AI/ML compiler stack, **PyBUDA** is the Python interface for TT-BUDA. TT-BUDA is the core technology; however, PyBUDA allows users to access and utilize TT-BUDA's features directly from Python. This includes directly importing model architectures and weights from **PyTorch** and other frameworks.



## Guide Overview

In this guide, we cover the steps for running the **YOLOX** model to detect objects in a video. The results are saved in a new video file with bounding boxes and labels overlaid on detected objects.

You will learn how to:
- Set up the appropriate libraries and environment
- Run YOLOX for video labeling
- Save the output with detected labels



### Step 1: Import Libraries

Ensure you have a Python environment with PyBUDA and YOLOX dependencies installed.

```python
# Import necessary libraries
import subprocess
import os
import cv2
import numpy as np
import pybuda
import requests
import torch
import logging
from queue import Queue
from pybuda._C.backend_api import BackendDevice
from yolox.data.data_augment import preproc as preprocess
from yolox.data.datasets import COCO_CLASSES
from yolox.exp import get_exp
from yolox.utils import demo_postprocess, multiclass_nms
```
<br>


### Step 2: Set PyBUDA Configuration and Device-Specific Settings
```python
def run_yolox_video(variant, video_path, output_path, batch_size=1):
    """
    Process all frames of a video file using YOLOX and generate a new video file containing object detection results.
    param variant: YOLOX model variant (e.g., yolox_nano, yolox_tiny, yolox_s, yolox_m, yolox_l, yolox_x)
    param video_path: input video file path
    param output_path: output video file path

    """
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s | %(levelname)s | %(message)s')
    logging.info("Starting YOLOX video processing.")

     # Set PyBuda configuration parameters
    compiler_cfg = pybuda.config._get_global_compiler_config()
    compiler_cfg.balancer_policy = "Ribbon"
    compiler_cfg.default_df_override = pybuda.DataFormat.Float16_b
    os.environ["PYBUDA_DECOMPOSE_SIGMOID"] = "1"
    logging.info("PyBuda configuration set.")

    # Device specific configurations
    available_devices = pybuda.detect_available_devices()
    logging.info(f"Available devices: {available_devices}")
    if available_devices:
        if available_devices[0] == BackendDevice.Wormhole_B0:
            if variant not in ["yolox_nano", "yolox_s"]:
                os.environ["PYBUDA_FORK_JOIN_BUF_QUEUES"] = "1"
                os.environ["PYBUDA_FORK_JOIN_EXPAND_OUTPUT_BUFFERS"] = "1"
                os.environ["PYBUDA_FORK_JOIN_SKIP_EXPANDING_BUFFERS"] = "1"
                os.environ["TT_BACKEND_TIMEOUT"] = "7200"  
```


<div style="border-left: 4px solid #3B82F6; background-color: #F0F8FF; padding: 10px; margin: 15px 0;">
    <strong style="color: #3B82F6;">Note:</strong> <span style="color: black;">Useful information that users should know, even when skimming content.</span>
</div>

### Load Model Weights and Prepare the Model

This section demonstrates how to load the YOLOX model weights and prepare the model using PyBUDA.

```python



